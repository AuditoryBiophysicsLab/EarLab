<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns:dt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882"
xmlns:st1="urn:schemas-microsoft-com:office:smarttags"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 11">
<meta name=Originator content="Microsoft Word 11">
<link rel=File-List href="Overview_files/filelist.xml">
<link rel=Edit-Time-Data href="Overview_files/editdata.mso">
<link rel=OLE-Object-Data href="Overview_files/oledata.mso">
<!--[if !mso]>
<style>
v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style>
<![endif]-->
<title>EarLab Overview</title>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="PlaceType"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="PlaceName"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="City"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="place"/>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>David C. Mountain</o:Author>
  <o:LastAuthor>David J. Anderson</o:LastAuthor>
  <o:Revision>2</o:Revision>
  <o:TotalTime>54</o:TotalTime>
  <o:LastPrinted>2004-11-03T20:31:00Z</o:LastPrinted>
  <o:Created>2005-03-30T01:52:00Z</o:Created>
  <o:LastSaved>2005-03-30T01:52:00Z</o:LastSaved>
  <o:Pages>1</o:Pages>
  <o:Words>2284</o:Words>
  <o:Characters>13020</o:Characters>
  <o:Company>Boston University Hearing Research Center</o:Company>
  <o:Lines>108</o:Lines>
  <o:Paragraphs>30</o:Paragraphs>
  <o:CharactersWithSpaces>15274</o:CharactersWithSpaces>
  <o:Version>11.5207</o:Version>
 </o:DocumentProperties>
 <o:CustomDocumentProperties>
  <o:_AdHocReviewCycleID dt:dt="float">987633820</o:_AdHocReviewCycleID>
  <o:_EmailSubject dt:dt="string">EarLab Overview V2 revised</o:_EmailSubject>
  <o:_AuthorEmail dt:dt="string">dcm@bu.edu</o:_AuthorEmail>
  <o:_AuthorEmailDisplayName dt:dt="string">Dave Mountain</o:_AuthorEmailDisplayName>
  <o:_ReviewingToolsShownOnce dt:dt="string"></o:_ReviewingToolsShownOnce>
 </o:CustomDocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:ValidateAgainstSchemas/>
  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>
  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>
  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>
  <w:Compatibility>
   <w:BreakWrappedTables/>
   <w:SnapToGridInCell/>
   <w:WrapTextWithPunct/>
   <w:UseAsianBreakRules/>
   <w:UseWord2002TableStyleRules/>
  </w:Compatibility>
  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>
 </w:WordDocument>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:LatentStyles DefLockedState="false" LatentStyleCount="156">
 </w:LatentStyles>
</xml><![endif]--><!--[if !mso]><object
 classid="clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D" id=ieooui></object>
<style>
st1\:*{behavior:url(#ieooui) }
</style>
<![endif]-->
<style>
<!--
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
h1
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:3.0pt;
	margin-left:0in;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:1;
	font-size:16.0pt;
	font-family:Arial;
	mso-font-kerning:16.0pt;}
h2
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:3.0pt;
	margin-left:0in;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:2;
	font-size:14.0pt;
	font-family:Arial;
	font-style:italic;}
h3
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:3.0pt;
	margin-left:0in;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:3;
	font-size:13.0pt;
	font-family:Arial;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
	{margin:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	tab-stops:center 3.0in right 6.0in;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;
	text-underline:single;}
 /* Page Definitions */
 @page
	{mso-footnote-separator:url("Overview_files/header.htm") fs;
	mso-footnote-continuation-separator:url("Overview_files/header.htm") fcs;
	mso-endnote-separator:url("Overview_files/header.htm") es;
	mso-endnote-continuation-separator:url("Overview_files/header.htm") ecs;}
@page Section1
	{size:8.5in 11.0in;
	margin:.75in .75in .75in .75in;
	mso-header-margin:.5in;
	mso-footer-margin:.5in;
	mso-even-footer:url("Overview_files/header.htm") ef1;
	mso-footer:url("Overview_files/header.htm") f1;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
-->
</style>
<!--[if gte mso 10]>
<style>
 /* Style Definitions */
 table.MsoNormalTable
	{mso-style-name:"Table Normal";
	mso-tstyle-rowband-size:0;
	mso-tstyle-colband-size:0;
	mso-style-noshow:yes;
	mso-style-parent:"";
	mso-padding-alt:0in 5.4pt 0in 5.4pt;
	mso-para-margin:0in;
	mso-para-margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-ansi-language:#0400;
	mso-fareast-language:#0400;
	mso-bidi-language:#0400;}
</style>
<![endif]--><!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="2050"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body lang=EN-US link=blue vlink=purple style='tab-interval:.5in'>

<div class=Section1>

<h1 style='text-align:justify'>EarLab: A Virtual Laboratory for Auditory
Experimentation</h1>

<p class=MsoNormal style='text-align:justify'><i style='mso-bidi-font-style:
normal'>David C. Mountain<sup>1</sup>, David Anderson<sup>1</sup>, Victor Vajda<sup>1</sup>,
and Glenn Bresnahan<sup>2</sup>.<o:p></o:p></i></p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><st1:PlaceName
w:st="on"><i style='mso-bidi-font-style:normal'><sup>1</sup>Boston</i></st1:PlaceName><i
style='mso-bidi-font-style:normal'> <st1:PlaceType w:st="on">University</st1:PlaceType>
<st1:PlaceName w:st="on">Hearing</st1:PlaceName> <st1:PlaceName w:st="on">Research</st1:PlaceName>
<st1:PlaceType w:st="on">Center</st1:PlaceType>, <st1:place w:st="on"><st1:PlaceName
 w:st="on"><sup>2</sup>Boston</st1:PlaceName> <st1:PlaceType w:st="on">University</st1:PlaceType></st1:place>
Scientific Computing and Visualization Group<o:p></o:p></i></p>

<h2 style='text-align:justify'>Introduction</h2>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'>EarLab is a
virtual laboratory that is intended to allow scientists from a wide range of
backgrounds to perform experiments on large-scale computational models of the
mammalian auditory pathways. The EarLab simulations are based on
interchangeable building blocks or <b style='mso-bidi-font-weight:normal'><i
style='mso-bidi-font-style:normal'>modules</i></b> each of which represents a
different component of the auditory system or sound-source, data-acquisition
and data-visualization systems. The modules are designed to run in a
distributed heterogeneous computing environment and to be configured at run
time. The physiological modules are designed to be species and subject
independent with species and/or subject dependent parameters loaded from a
parameter database at run time.<span style='mso-spacerun:yes'>  </span>The
current system includes modules that represent sound sources, propagation
between the source and the tympanic membrane, and the subthalamic auditory
pathways. These modules can all run on a single CPU (Desktop EarLab) or they
can be distributed across many heterogeneous CPUs connected by a local or a
wide-area network (Distributed EarLab).<span style='mso-spacerun:yes'> 
</span>A web-based demonstration is also available (<a
href="http://earlab.bu.edu/modeling/online/Default.aspx">http://earlab.bu.edu/modeling/online/Default.aspx</a>).</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'>The modular
approach allows one to easily upgrade portions of a model independently of
other portions of the model.<span style='mso-spacerun:yes'>  </span>The modular
approach also allows one to choose between modules that use a high level of
abstraction to reduce computation time and more accurate but slower modules
that include considerable biophysical detail.<span style='mso-spacerun:yes'> 
</span>The software architecture is designed to be very general purpose so that
it will be possible to combine models of other physiological systems such as
the cardiovascular systems with models of the nervous system.</p>

<h2>Auditory Anatomy</h2>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'>The current
scope of the EarLab project is limited to the subthalamic pathways.<span
style='mso-spacerun:yes'>  </span>Figure 1 illustrates the major ascending
auditory pathways.</p>

<p class=MsoNormal>The external and middle ears serve to couple sound to the
cochlea where it is transduced and encoded into firing patterns in the auditory
nerve fibers.<span style='mso-spacerun:yes'>  </span>The acoustic information
is then sent to the cochlear nucleus, the first group of neurons in the
brainstem to receive auditory input.<span style='mso-spacerun:yes'>  </span>The
cochlear nucleus has 3 major and several minor subdivisions.<span
style='mso-spacerun:yes'>  </span>The major subdivisions, dorsal cochlear nucleus
(DCN), posteroventral cochlear nucleus (PVCN) and the anteroventral cochlear
nucleus (AVCN) appear to be specialized for the extraction of different
acoustic features.<span style='mso-spacerun:yes'>  </span>The AVCN enhances the
neural representation of the fine structure in the acoustic stimulus and
provides input to the medial superior olive (MSO), a group of neurons
specialized for the analysis interaural time differences (ITD).<span
style='mso-spacerun:yes'>  </span>The ITD is the major cue for estimating the
azimuth of low-frequency sound sources.<span style='mso-spacerun:yes'> 
</span>The AVCN also provides input to the medial nucleus of the trapezoid body
(MTB) and the lateral superior olive (LSO).<span style='mso-spacerun:yes'> 
</span>The MTB and LSO form a processing unit that is specialized for the
analysis of interaural sound level differences (ILD).<span
style='mso-spacerun:yes'>  </span>The ILD is the major cure for estimating the
azimuth of high-frequency sound sources.<span style='mso-spacerun:yes'> 
</span>The DCN appears to be specialized for the analysis of spectral features
that are believed to be important cues for estimating sound source
elevation.<span style='mso-spacerun:yes'>  </span>Because the DCN, LSO and MSO
systems appear to be processing acoustic cues related to sound source
direction, the are often referred to as the “where” pathways.<span
style='mso-spacerun:yes'>  </span>In contrast, the PVCN appears to be more
involved in analyzing acoustic temporal features that are important for sound
source classification and may be part of the “what” pathway.</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><!--[if gte vml 1]><o:wrapblock><v:shapetype id="_x0000_t75"
  coordsize="21600,21600" o:spt="75" o:preferrelative="t" path="m@4@5l@4@11@9@11@9@5xe"
  filled="f" stroked="f">
  <v:stroke joinstyle="miter"/>
  <v:formulas>
   <v:f eqn="if lineDrawn pixelLineWidth 0"/>
   <v:f eqn="sum @0 1 0"/>
   <v:f eqn="sum 0 0 @1"/>
   <v:f eqn="prod @2 1 2"/>
   <v:f eqn="prod @3 21600 pixelWidth"/>
   <v:f eqn="prod @3 21600 pixelHeight"/>
   <v:f eqn="sum @0 0 1"/>
   <v:f eqn="prod @6 1 2"/>
   <v:f eqn="prod @7 21600 pixelWidth"/>
   <v:f eqn="sum @8 21600 0"/>
   <v:f eqn="prod @7 21600 pixelHeight"/>
   <v:f eqn="sum @10 21600 0"/>
  </v:formulas>
  <v:path o:extrusionok="f" gradientshapeok="t" o:connecttype="rect"/>
  <o:lock v:ext="edit" aspectratio="t"/>
 </v:shapetype><v:shape id="_x0000_s1026" type="#_x0000_t75" style='position:absolute;
  margin-left:0;margin-top:0;width:195.6pt;height:251.4pt;z-index:1;
  mso-position-horizontal:center;mso-position-horizontal-relative:page;
  mso-position-vertical:top;mso-position-vertical-relative:line'
  o:allowoverlap="f">
  <v:imagedata src="Overview_files/image001.jpg" o:title="Brain_Diagram"/>
  <w:wrap type="topAndBottom" anchorx="page"/>
 </v:shape><![endif]--><![if !vml]><img width=261 height=335
 src="Overview_files/image002.jpg" v:shapes="_x0000_s1026"><![endif]><!--[if gte vml 1]></o:wrapblock><![endif]--><br
style='mso-ignore:vglayout' clear=ALL>
</p>

<p class=MsoNormal align=center style='text-align:center'>Figure 1</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>The outputs of all of these brainstem auditory structures
eventually reach the inferior colliculus (IC) in the midbrain either directly
or via the nuclei of the lateral lemniscus (NLL).<span
style='mso-spacerun:yes'>  </span>The available data indicate that at least
some of the features pathways that begin in the brainstem continue to be
segregated at the level of the IC.<span style='mso-spacerun:yes'>  </span>The
IC in turn projects to the medial geniculate body (MGB) in the thalamus and the
MGB passes the auditory information on to the auditory cortex.</p>

<h2 style='text-align:justify'>Methods</h2>

<h3>Experiments</h3>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><i style='mso-bidi-font-style:normal'>Experiments</i></b>
consist of multiple runs where a <b style='mso-bidi-font-weight:normal'><i
style='mso-bidi-font-style:normal'>run</i></b> is a single simulation on a
specific <b style='mso-bidi-font-weight:normal'><i style='mso-bidi-font-style:
normal'>model</i></b> with a single set of <b style='mso-bidi-font-weight:normal'><i
style='mso-bidi-font-style:normal'>parameters</i></b>. A model consists of a
choice of modules and their interconnections. A <b style='mso-bidi-font-weight:
normal'><i style='mso-bidi-font-style:normal'>model configuration file</i></b>
is used to specify the modules to be used, the files containing the module
parameters (<b style='mso-bidi-font-weight:normal'><i style='mso-bidi-font-style:
normal'>parameter files</i></b>), the dimensionality of the module outputs, and
the source of module inputs.<span style='mso-spacerun:yes'>  </span>Parameters
for the modules can all be listed in a single file or each module can have its
own parameter file. In general, there will be multiple pre-computed parameter
sets available for each module in order to represent the differences between
different species.<span style='mso-spacerun:yes'>  </span>These parameters will
be stored in a database so that the experimenter can choose the species to be
represented in a simulation and the parameters automatically loaded.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'>Experiments
typically involve the systematic variation of one or more parameters.<span
style='mso-spacerun:yes'>  </span>For example, if the acoustic stimulus is a
sinusoid, the experimenter might wish to execute a series of runs where the
parameters associated with the neural modules are kept fixed while the stimulus
frequency and amplitude are systematically varied. These parameters might be
supplied in a table or they might be computed by specifying the number of
parameter steps and an upper and lower bound. Alternatively, the stimulus
parameters might be kept fixed and one or more parameter sets for the neural
modules are varied.</p>

<h3 align=center style='text-align:center'>System Software Architecture of
Desktop EarLab</h3>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:390pt;height:392.25pt'>
 <v:imagedata src="Overview_files/image003.png" o:title="Desktop Earlab Block Diagram"/>
</v:shape><![endif]--><![if !vml]><img border=0 width=520 height=523
src="Overview_files/image004.jpg" v:shapes="_x0000_i1025"><![endif]></p>

<p class=MsoNormal align=center style='text-align:center'>Figure 2</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal style='text-align:justify'>The overall simulation
architecture is designed to be able to represent any physiological system or
group of systems. The software system (Figure 2) is divided into five main layers:
the User Interface Layer, the Presentation Layer, the Control Layer, the
Transport Layer, and the Module Layer. The User Interface layer is a custom
written user interface, consisting of the EarLab Experiment Manager and the
EarLab Simulator, working cooperatively. The Presentation Layer mediates
between the User Interface and the Control Layer, passing user requests to the
Control Layer and presenting Control Layer responses to those requests back to
the user. The Control Layer manages the simulation environment, maintaining
detailed representations of the model states as simulations are created,
loaded, executed, and examined. The Transport Layer mediates all data transfer
between the Control Layer and the Module Layer and between elements of the
Module Layer.<span style='mso-spacerun:yes'>  </span>In Desktop EarLab, the
Transport layer is a simple double-buffering scheme, because the inter-module
data communication all happens within the boundaries of a single process on a
single CPU.<span style='mso-spacerun:yes'>  </span>For Distributed EarLab, the
Transport Layer is based on the DAFFIE architecture (<a
href="http://scv.bu.edu/SCV/DAFFIE/outline.html">http://scv.bu.edu/SCV/DAFFIE/outline.html</a>)
developed at <st1:place w:st="on"><st1:PlaceName w:st="on">Boston</st1:PlaceName>
 <st1:PlaceType w:st="on">University</st1:PlaceType></st1:place> for
distributed virtual environments.</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal style='text-align:justify'>The Transport Layer is configured
at the beginning of each run to reflect the desired connectivity between Module
Layer entities. Once the simulation has been started, the Modules communicate
simulation data among themselves mediated by the Transport Layer, as well as
communicating status information (current state, computation progress, etc) to
the Control Layer, which may then be further summarized and presented to the
User as information regarding overall progress in the current simulation.<span
style='mso-spacerun:yes'>  </span>The transport layer supports data that
represent sampled continuous waveforms such as acoustic signals and cell
membrane potentials as well as event data such as the times of occurrence for action
potentials in neurons.</p>

<h3 style='text-align:justify'>Time is segmented into frames</h3>

<p class=MsoNormal style='text-align:justify'>Long acoustic waveforms such as
speech are split into small pieces called frames (Figure 3).<span
style='mso-spacerun:yes'>  </span>Each module performs its computations one
frame at a time. Modules compute asynchronously during a frame and then are
synchronized at the end of each frame.<span style='mso-spacerun:yes'>  </span>A
single frame can contain a single time step or many time steps.<span
style='mso-spacerun:yes'>  </span>The framing approach not only facilitates
distributed simulations but also can be used to integrate information from
modules that are operating on different time scales.<span
style='mso-spacerun:yes'>  </span>For example, one module could be calculating
for one time step per frame while another module in the same simulation could
be running at a finer time scale and computing multiple time steps per frame.</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'><!--[if gte vml 1]><v:shape
 id="_x0000_i1026" type="#_x0000_t75" style='width:210pt;height:351pt'>
 <v:imagedata src="Overview_files/image005.jpg" o:title="Frames"/>
</v:shape><![endif]--><![if !vml]><img border=0 width=280 height=468
src="Overview_files/image006.jpg" v:shapes="_x0000_i1026"><![endif]></p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'>Figure 3</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<h3 style='text-align:justify'>Modules are Based on Anatomy</h3>

<p class=MsoNormal><b style='mso-bidi-font-weight:normal'><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b style='mso-bidi-font-weight:normal'>Example Modules:<o:p></o:p></b></p>

<p class=MsoNormal><b style='mso-bidi-font-weight:normal'><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=external+ear+AND+neurosci%5bbook%5d+AND+231514%5buid%5d&amp;rid=neurosci.section.891">External
Ear</a>:</b> the external ear or pinna in conjunction with the head acts as a
direction-dependent filter.<span style='mso-spacerun:yes'>  </span>The
resulting head-related transfer function (HRTF) provides cues that are
important for localization. <span style='mso-spacerun:yes'> </span>Physical
modeling of the pinna is difficult so the HRTF is typically implemented as a finite
impulse response (FIR) digital filter with filter coefficients that depend on
sound source location.<span style='mso-spacerun:yes'>  </span>Experimentally
determined filter coefficients are typically stored in a database to allow
simulation of multiple species and multiple subjects.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=middle+ear+AND+neurosci%5bbook%5d+AND+231515%5buid%5d&amp;rid=neurosci.section.893">Middle
Ear</a>:</b><span style='mso-spacerun:yes'>  </span>the middle ear acts as a
high-pass filter with a corner frequency that varies greatly across
species.<span style='mso-spacerun:yes'>  </span>The middle ear transfer
function (METF) is believed to be a major factor in a given species’ ability to
hear low-frequency sound.<span style='mso-spacerun:yes'>  </span>The METF is
typically implemented as an infinite impulse response (IIR) digital filter.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=inner+ear+AND+neurosci%5bbook%5d+AND+231524%5buid%5d&amp;rid=neurosci.section.894">Cochlear
Mechanics</a>:</b> the basilar membrane (BM) and associated outer hair cells
within the cochlea act like a bank of band-pass filters whose outputs drive the
inner hair cells.<span style='mso-spacerun:yes'>  </span>The mechanics of the
cochlea are quite complex so in order to reduce computational complexity, these
mechanics are often approximated with linear or non-linear digital band-pass
filters.<span style='mso-spacerun:yes'>  </span>Currently, we have only
implemented the filter-bank approach, but full hydromechanical cochlear models
will be included in the future.<span style='mso-spacerun:yes'>  </span>The
basilar membrane module receives its input from the middle ear and provides
multiple channels of output, each channel representing a specific location
along the length of the basilar membrane.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=two+kinds+of+hair+cells+AND+neurosci%5bbook%5d+AND+231528%5buid%5d&amp;rid=neurosci.section.907">Inner
Hair Cell</a>s (IHC):</b><span style='mso-spacerun:yes'>  </span>the inner hair
cell transduction process produces a receptor current that is a half-wave
rectified version of the mechanical stimulus.<span style='mso-spacerun:yes'> 
</span>The resulting receptor potential is a low-pass filtered version of the
receptor current due to the RC nature of the cell membrane impedance.<span
style='mso-spacerun:yes'>  </span>The inner hair cell model receives an array
of inputs from the basilar membrane module and produces an array of outputs
representing the receptor potentials in a population of IHCs.<span
style='mso-spacerun:yes'>  </span>The number of IHCs is generally made to equal
to the number of locations simulated in the BM module. Typical mammals have
1,000-3000 IHCs but often fewer numbers are simulated in order to reduce
computation times.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=auditory+nerve+AND+neurosci%5bbook%5d+AND+231531%5buid%5d&amp;rid=neurosci.section.908">Auditory
Nerve</a> (AN):</b><span style='mso-spacerun:yes'>  </span>The synapse between
the inner hair cells and the auditory nerve fibers exhibit significant
adaptation.<span style='mso-spacerun:yes'>  </span>This adaptation is thought
to be due, at least in part, to depletion of synaptic vesicles and functions as
an automatic gain control. The auditory nerve module receives an array of
inputs from the inner hair cell module.<span style='mso-spacerun:yes'> 
</span>The module output represents a population of AN fibers and in general
there can be more than one AN fiber for each IHC.<span
style='mso-spacerun:yes'>  </span>Typical mammals have 10,000-50,000 AN fibers
but typically fewer numbers are simulated in order to speed up the simulations.</p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=cochlear+nucleus+AND+neurosci%5bbook%5d+AND+231533%5buid%5d&amp;rid=neurosci.section.911">AnteroVentral
Cochlear Nucleus</a> (AVCN):</b> the bushy cells in AVCN receive input from several
AN fibers that are tuned to similar acoustic frequencies.<span
style='mso-spacerun:yes'>  </span>They appear to act as coincidence detectors
and their function appears to be to reduce temporal jitter present in the
timing of AN action potentials. </p>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'><b
style='mso-bidi-font-weight:normal'><a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Search&amp;db=books&amp;doptcmdl=GenBookHL&amp;term=two+ears+AND+neurosci%5bbook%5d+AND+231536%5buid%5d&amp;rid=neurosci.section.913">Medial
Superior Olive</a> (MSO):</b><span style='mso-spacerun:yes'>  </span>the bipolar
cells in the MSO receive input from bushy cells in both cochlear nuclei and act
as coincidence detectors. Their response is sensitive to interaural time delay
and is believed to play a key role in sound localization.</p>

<b><span style='font-size:13.0pt;font-family:Arial;mso-fareast-font-family:
"Times New Roman";mso-ansi-language:EN-US;mso-fareast-language:EN-US;
mso-bidi-language:AR-SA'><br clear=all style='page-break-before:always'>
</span></b>

<h3 style='text-align:justify'>Species Dependent Parameters</h3>

<p class=MsoNormal style='margin-bottom:12.0pt;text-align:justify'>Module
parameters that are expected to vary across species or across individual
subjects are specified externally and loaded by the modules at run time.<span
style='mso-spacerun:yes'>  </span>An example of a set of species dependent
parameters is the parameter set that is used to compute the cochlear
frequency-place map.<span style='mso-spacerun:yes'>  </span>In 1961, Don
Greenwood (J. Acoust. Soc. Am. 33:1344-1356) developed a function that relates
the characteristic frequency (CF) of any location along the length of the basilar
membrane to the distance (x) of that location from the apex. </p>

<p class=MsoNormal align=center style='margin-bottom:12.0pt;text-align:center'>CF
= A (10 <sup>ax/L</sup> - K)</p>

<p class=MsoNormal style='text-align:justify'>where:<span style='mso-tab-count:
2'>             </span>A<span style='mso-tab-count:1'>         </span>=<span
style='mso-tab-count:1'>          </span>constant that controls the
high-frequency limit of the map (Hz)<span style='mso-tab-count:1'>        </span></p>

<p class=MsoNormal style='text-align:justify'><span style='mso-tab-count:2'>                        </span>a<span
style='mso-tab-count:1'>          </span>=<span style='mso-tab-count:1'>          </span>constant
that controls the slope of the map<span style='mso-tab-count:1'>    </span></p>

<p class=MsoNormal style='text-align:justify'><span style='mso-tab-count:2'>                        </span>L<span
style='mso-tab-count:1'>          </span>=<span style='mso-tab-count:1'>          </span>cochlear
length in mm<span style='mso-tab-count:1'> </span></p>

<p class=MsoNormal style='text-align:justify'><span style='mso-tab-count:2'>                        </span>K<span
style='mso-tab-count:1'>         </span>=<span style='mso-tab-count:1'>          </span>constant
that controls low-frequency behavior of the map</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'><!--[if gte vml 1]><v:shape
 id="_x0000_i1027" type="#_x0000_t75" style='width:459pt;height:193.5pt' o:ole="">
 <v:imagedata src="Overview_files/image007.emz" o:title=""/>
</v:shape><![endif]--><![if !vml]><img border=0 width=612 height=258
src="Overview_files/image008.gif" v:shapes="_x0000_i1027"><![endif]><!--[if gte mso 9]><xml>
 <o:OLEObject Type="Embed" ProgID="CorelDRAW.Graphic.10" ShapeID="_x0000_i1027"
  DrawAspect="Content" ObjectID="_1173634720">
 </o:OLEObject>
</xml><![endif]--></p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<h2 style='text-align:justify'>Results</h2>

<p class=MsoNormal style='text-align:justify'>Figure 4 shows sample output for
a model that includes structures up to the auditory nerve with a speech
waveform as input.<span style='mso-spacerun:yes'>  </span>Waveforms for the 689
Hz channel are shown as well as 2-D color plots showing the total model output.
The input waveform was “<st1:place w:st="on"><st1:City w:st="on">Boston</st1:City></st1:place>”
as shown in Figure 3.<span style='mso-spacerun:yes'>  </span>The sound source
was simulated using a data-source module that reads prerecorded audio files and
scales the signal to a desired peak sound level.<span
style='mso-spacerun:yes'>  </span>Simulation output from the BM, IHC, and AN
modules was stored using data-sink modules that capture simulation module
output on a frame by frame basis and write the output to disk in standard
format for offline analysis.<span style='mso-spacerun:yes'>  </span>Data can be
stored either in raw form or in a multi-resolution format that allows users to
quickly change time scales when viewing the data.<span
style='mso-spacerun:yes'>  </span>The data-sink modules also support real-time
monitoring of simulations by remote users over the internet.</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'><!--[if gte vml 1]><v:shape
 id="_x0000_i1028" type="#_x0000_t75" style='width:7in;height:394.5pt'>
 <v:imagedata src="Overview_files/image009.jpg" o:title="AN_model"/>
</v:shape><![endif]--><![if !vml]><img border=0 width=672 height=526
src="Overview_files/image010.jpg" v:shapes="_x0000_i1028"><![endif]></p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal align=center style='text-align:center'>Figure 4</p>

<p class=MsoNormal style='text-align:justify'><o:p>&nbsp;</o:p></p>

</div>

</body>

</html>
